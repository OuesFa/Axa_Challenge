{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> <a> Apprentissage </a> </h1>\n",
    "<h2> Pour le Challenge Axa </h2>\n",
    "<h3> Axa Graduate Program Février 2017 </h3>\n",
    "<i> Author : Paul Tran </i> <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sommaire\n",
    "1. [Préliminaires](#préliminaires)\n",
    "    1. [Introduction](#introduction)\n",
    "    2. [Imports](#imports)\n",
    "    3. [Chargement de la donnée](#load)\n",
    "2. [Data Management](#management)\n",
    "    1. [Régression sur valeurs manquantes](#nan)\n",
    "    2. [Traitement outliers](#traitement)\n",
    "    3. [Train, Validation & Test](#split)\n",
    "\n",
    "3. [Machine Learning](#ML)\n",
    "    1. [Méthode Linéaire](#linéaire)\n",
    "        1. [Régression Linéaire simple](#lr)\n",
    "        2. [Lasso](#lasso)\n",
    "    2. [Méthodes ensemblistes](#ensemblistes)\n",
    "        1. [Random Forest](#RF)\n",
    "        2. [Extra Trees](#ET)\n",
    "        3. [XG Boost](#XGB)\n",
    "4. [Datamagement](#datamanagement)\n",
    "    1. [Valeurs Manquantes](#missing)\n",
    "    2. [Encodage disjonctif complet](#dummies)\n",
    "    3. [Clustering](#kmeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Préliminaires  <a name=\"préiliminaires\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Introduction :  <a name=\"introduction\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette partie nous allons gérer chercher à traiter plus intelligemment la donnée:\n",
    "- remplir les valeurs manquantes plus intelligemment\n",
    "- retraiter les extremas (outliers)\n",
    "- tenter de dériver des features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Précautions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce notebook prend beaucoup de temps à être exécuté dans son intégralité du fait des apprentissages successifs qui sont lancés. Il est conseiller de ne pas l'exécuter et d'observer directement les résultats des cellules. De plus ce notebook sera très similaire au précédant en terme de machine learning, seule la partie préprocessing change.\n",
    "\n",
    "En effet, le gain de performance est bien plus important en traitant la donnée qu'en tunant le modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Imports des librairies utiles <a name=\"imports\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 300)\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#pour la partie data viz\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#import bokeh\n",
    "#from bokeh.io import output_notebook\n",
    "#output_notebook()\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Chargement de la donnée  <a name=\"load\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Nous chargeons les données traitées préalablement (depuis le Notebook Ch1), cela évite de devoir réécrire tout le code du premier Notebook dans celui-ci.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%store -r data_raw01\n",
    "#Récupération de la variable data depuis le Notebook stocké précédemment.\n",
    "#Cette fois ci, nous conservons les départements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Nous n'observons que le début de la table pour s'assurer qu'elle s'est bien chargée.\n",
    "data_raw01.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Data management  <a name=\"management\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - ### Régression sur les données manquantes <a name=\"nan\"></a>\n",
    "\n",
    "***Premier traitement, devenu classique : nous listons les variables dotées de valeurs manquantes (des NAN en l'occurence).***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Vérifier les colonnes concernées [possédant des NaN]\n",
    "for element in data_raw01.isnull().sum().index:\n",
    "    if data_raw01[element].isnull().sum()!=0:\n",
    "        print (element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***On cherche les variables catégorielles qui ne possèdent pas de valeurs nulles afin de les \"dummiser\" (encodage disjonctif complet).***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Vérifier les colonnes qui sont de type objet\n",
    "list_categorical=[]\n",
    "for element in data_raw01.columns:\n",
    "    if data_raw01[element].dtype==np.object:\n",
    "        list_categorical.append(element)\n",
    "        print('La variable %s est de type Object.' %(element))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_raw01 = pd.get_dummies(data_raw01, columns=['Categorie socio professionnelle',\n",
    "                                                 'Type de vehicule'],drop_first=True)\n",
    "data_raw01.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print pour s'assurer du résultat\n",
    "data_raw01.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***On va désormais régresser les variables numériques manquantes à partir de la matrice de données complète.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Nous régressons sur les variables contenant des NaN\n",
    "test = data_raw01\n",
    "l=['Age','Prime mensuelle']\n",
    "for element in l:\n",
    "    temp = test.copy(deep=True)\n",
    "    del temp['Benefice net annuel']\n",
    "    del temp['Marque']\n",
    "    for e in [x for x in l if x!=element]:\n",
    "        del temp[e]\n",
    "    columns1 = list(temp.columns)\n",
    "    columns1.remove(element)\n",
    "    #retrouver les indices des lignes avec des nan pour la variable en question\n",
    "    index = data_raw01[element].index[data_raw01[element].apply(np.isnan)]\n",
    "    #drop les lignes\n",
    "    temp = temp.drop(temp.index[index])\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(temp[columns1], temp[element])\n",
    "\n",
    "    for idx in index:\n",
    "        X= data_raw01.iloc[idx][columns1]\n",
    "        y_lr = lr.predict(X.values.reshape(1, -1))\n",
    "        #print(y_lr[0])\n",
    "        data_raw01.ix[idx,element]=y_lr[0]\n",
    "    print('Régression achevée pour la variable : %s.') %(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for element in data_raw01.isnull().sum().index:\n",
    "    if data_raw01[element].isnull().sum()==0 and element=='Age' and element!='Benefice net annuel':\n",
    "        #data_raw01[element] = data_raw01[element].fillna(data_raw01[element].median())\n",
    "        print(\"L'age ne comporte plus de champs vides.\")\n",
    "    if data_raw01[element].isnull().sum()==0 and element=='Prime mensuelle' and element!='Benefice net annuel':\n",
    "        #data_raw01[element] = data_raw01[element].fillna(data_raw01[element].median())\n",
    "        print(\"La Prime mensuelle ne comporte plus de champs vides.\")\n",
    "    if data_raw01[element].dtype==np.object:\n",
    "        data_raw01[element]=data_raw01[element].fillna(data_raw01[element].describe().top)#remplacer par sa valeur top\n",
    "\n",
    "data_raw01 = pd.get_dummies(data_raw01, columns=['Marque'],drop_first=True)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - ### Gestion des outliers <a name=\"traitement\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_raw01[data_raw01['Salaire annuel']>90000]['Salaire annuel'] #38580"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Cette modification est due à une forte interprétation métier\n",
    "#Le choix se portant sur l'age car il est juste impossible ou aberrant d'avoir un client à 0 an\n",
    "    #ou a 198 ans\n",
    "def traiter_age(x):\n",
    "    if x<18:\n",
    "        return 18\n",
    "    if x>99:\n",
    "        return 99\n",
    "    return x\n",
    "\n",
    "def traiter_salaire(x,threshold):\n",
    "    if x>threshold:\n",
    "        return threshold\n",
    "    return x\n",
    "\n",
    "data_raw01['Age'] = data_raw01['Age'].apply(lambda x: traiter_age(x))\n",
    "#data_raw01['Salaire annuel'] = data_raw01['Salaire annuel'].apply(lambda x: traiter_salaire(x,100000))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "#preprocessing.normalize(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Train, Validation & Test set  <a name=\"split\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***La donnée étant déjà traité de façon la plus simple depuis le premier Notebook, nous nous contentons pour notre première approche \"naïve\" de simplement séparer notre base de train afin de pouvoir évaluer le modèle d'apprentissage.***\n",
    "\n",
    "***Nous la considérons \"naïve\", dans la mesure où nous nous sommes seulement contentés de remplacer les NaN par la moyenne des valeurs existantes (pour les variables quantitatives). Le traitement des données est indispensable afin que le modèle d'apprentissage accepte le format des données en entrée.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def simple_split(X):\n",
    "    to_drop=['Benefice net annuel'] #on enlève la cible\n",
    "    T=X[1000:]\n",
    "    X=X[0:1000]  \n",
    "    return X.drop(to_drop,axis=1),X['Benefice net annuel'],T.drop(to_drop,axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "X,y,T=simple_split(data_raw01.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# II. Machine learning  <a name=\"ML\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here comes the juicy part!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Avant d'entrer dans le coeur du machine learning, il est nécessaire de définir une métrique d'évaluation.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "# RMSE :root-mean-square error\n",
    "def RMSE(y_true, y_pred): \n",
    "    return sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "\n",
    "# Mean Absolute Percentage Error\n",
    "def mape_error(y_true, y_pred): \n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Nous allons scinder la base de train afin de pouvoir évaluer le modèle, par l'intermédiaire de la méthode de validation croisée.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random_state = 42 #set seed pour l'ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X0_train,X0_test,y0_train,y0_test=train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "\n",
    "import sklearn\n",
    "from sklearn.cross_validation import train_test_split, KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn import grid_search\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## A. Méthode Linéaire  <a name=\"linéaire\"></a>\n",
    "    - Approche la plus naïve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#Apprentissage:\n",
    "lr = LinearRegression()\n",
    "lr.fit(X0_train, y0_train)\n",
    "\n",
    "#Prédiction:\n",
    "y_lr = lr.predict(X0_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Evaluation du modèle :\n",
    "print(\"L'erreur type (RMSE) est de %s\") %(RMSE(y0_test, y_lr))\n",
    "print(\"La moyenne absolue de pourcentage d'erreur est de %s %%\") %(mape_error(y0_test, y_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## A. Méthode Linéaire  <a name=\"lr\"></a>\n",
    "    - Approche polynomiale et cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def k_fold_cross_val_poly(folds, degrees, X, y):\n",
    "    #fonction qui calcule les scores RMSE pour K-fold et d degré polynomial\n",
    "    n = len(X)\n",
    "    kf = KFold(n, n_folds=folds,random_state=42)\n",
    "    #creation du dico qui retient les scores RMSE\n",
    "    kf_dict = dict([(\"fold_%s\" % i,[]) for i in range(1, folds+1)])\n",
    "    fold = 0\n",
    "    for train_index, test_index in kf:\n",
    "        fold += 1\n",
    "        #print \"Fold: %s\" % fold\n",
    "        X_train, X_test = X.ix[train_index], X.ix[test_index]\n",
    "        y_train, y_test = y.ix[train_index], y.ix[test_index]\n",
    "        # Incrémente le dégré polynomial\n",
    "        for d in range(1, degrees+1):\n",
    "            #print \"Degree: %s\" % d\n",
    "            # Model & fit\n",
    "            polynomial_features = PolynomialFeatures(\n",
    "                degree=d, include_bias=False\n",
    "            )\n",
    "            linear_regression = LinearRegression()\n",
    "            model = Pipeline([\n",
    "                (\"polynomial_features\", polynomial_features),\n",
    "                (\"linear_regression\", linear_regression)\n",
    "            ])\n",
    "            model.fit(X_train, y_train)\n",
    "            # Calcul du RMSE\n",
    "            y_pred = model.predict(X_test)\n",
    "            test_rmse = RMSE(y_test, y_pred)\n",
    "            #print test_rmse\n",
    "            kf_dict[\"fold_%s\" % fold].append(test_rmse) #stockage dans le dict\n",
    "        # Transform en np.array pour le moyenner\n",
    "        kf_dict[\"fold_%s\" % fold] = np.array(kf_dict[\"fold_%s\" % fold])\n",
    "    #Dans le dico\n",
    "    kf_dict[\"avg\"] = np.zeros(degrees)\n",
    "    for i in range(1, folds+1):\n",
    "        kf_dict[\"avg\"] += kf_dict[\"fold_%s\" % i]\n",
    "    kf_dict[\"avg\"] /= float(folds)\n",
    "    return kf_dict\n",
    "\n",
    "def plot_test_error_curves_kf(kf_dict, folds, degrees):\n",
    "    #fonction qui permet de visualiser les résultats calculés par la fonction ci-dessus\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    ds = range(1, degrees+1)\n",
    "    for i in range(1, folds+1):\n",
    "        ax.plot(ds, kf_dict[\"fold_%s\" % i], lw=2, label='Test RMSE - Fold %s' % i)\n",
    "    ax.plot(ds, kf_dict[\"avg\"], linestyle='--', color=\"black\", lw=3, label='Avg Test RMSE')\n",
    "    ax.legend(loc=0)\n",
    "    ax.set_xlabel('Degree of Polynomial Fit')\n",
    "    ax.set_ylabel('RMSE')\n",
    "    #ax.set_ylim([0.0, 4.0])\n",
    "    fig.set_facecolor('white')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot la courbe d'erreur k-fold CV set\n",
    "folds = 8\n",
    "degrees = 3\n",
    "kf_dict = k_fold_cross_val_poly(folds, degrees, X, y)\n",
    "plot_test_error_curves_kf(kf_dict, folds, degrees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Le régresseur polynomial atteint une valeur moyenne minimale RMSE : %s pour un degré %s'\n",
    "      %(kf_dict['avg'].min(),kf_dict['avg'].argmin()+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- > *** Intreprétation: on décide de choisir un degré polynomial de 2. ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sélection du model:\n",
    "\n",
    "lr = LinearRegression() #init\n",
    "polynomial_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "# Création du pipeline\n",
    "pipe = Pipeline([(\"polynomial_features\", polynomial_features),\n",
    "        (\"linear_regression\", lr)])\n",
    "grid = dict(linear_regression__normalize=[False,True]) #espace de paramètre de la régression\n",
    "                                                        # choix sur la normalisation\n",
    "model = GridSearchCV(pipe,param_grid=grid,cv=8)\n",
    "model.fit(X0_train, y0_train)\n",
    "#predict\n",
    "y_lr = model.predict(X0_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Evaluation du modèle :\n",
    "print(\"L'erreur type (RMSE) est de %s\") %(RMSE(y0_test, y_lr))\n",
    "print(\"La moyenne absolue de pourcentage d'erreur est de %s %%\") %(mape_error(y0_test, y_lr))\n",
    "print(\"Meilleur model utilisant %s.\" % ( model.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "***Nous obtenons un pourcentage d'erreur d'environ 13 % et une RMSE de 1.06 légèrement ce qui est légèrement mieux que la régression sur la donnée brute avec minimum de traitement. On constate donc un gain à régresser les valeurs manquantes et à retraiter les valeurs liées à l'Age.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## A-bis. Méthode Linéaire Lasso   <a name=\"lasso\"></a>\n",
    "    - Approche polynomiale et cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lasso = Lasso(random_state=42)\n",
    "alphas = np.logspace(-4, -0.5, 30)\n",
    "\n",
    "scores = list()\n",
    "scores_std = list()\n",
    "\n",
    "n_folds = 3\n",
    "\n",
    "for alpha in alphas:\n",
    "    lasso.alpha = alpha\n",
    "    this_scores = cross_val_score(lasso, X, y, cv=n_folds, n_jobs=1)\n",
    "    scores.append(np.mean(this_scores))\n",
    "    scores_std.append(np.std(this_scores))\n",
    "\n",
    "scores, scores_std = np.array(scores), np.array(scores_std)\n",
    "\n",
    "plt.figure().set_size_inches(8, 6)\n",
    "plt.semilogx(alphas, scores)\n",
    "\n",
    "# plot error lines showing +/- std. errors of the scores\n",
    "std_error = scores_std / np.sqrt(n_folds)\n",
    "\n",
    "plt.semilogx(alphas, scores + std_error, 'b--')\n",
    "plt.semilogx(alphas, scores - std_error, 'b--')\n",
    "\n",
    "# alpha=0.2 controls the translucency of the fill color\n",
    "plt.fill_between(alphas, scores + std_error, scores - std_error, alpha=0.2)\n",
    "\n",
    "plt.ylabel('CV score +/- std error')\n",
    "plt.xlabel('alpha')\n",
    "plt.axhline(np.max(scores), linestyle='--', color='.5')\n",
    "plt.xlim([alphas[0], alphas[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lasso = Lasso(random_state=42)\n",
    "alphas = np.logspace(-4, -0.5, 30)\n",
    "\n",
    "#feature polynomial d'ordre 2\n",
    "polynomial_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X0_train_ = polynomial_features.fit_transform(X0_train)\n",
    "X0_test_ = polynomial_features.fit_transform(X0_test)\n",
    "\n",
    "#list\n",
    "rmse = list()\n",
    "MAPE = list()\n",
    "\n",
    "n_folds = 3\n",
    "\n",
    "for alpha in alphas:\n",
    "    #lasso = LassoCV(random_state=42, cv=10)\n",
    "    lasso.alpha = alpha\n",
    "    lasso.fit(X0_train_,y0_train)\n",
    "    y_lr=lasso.predict(X0_test_)\n",
    "    rmse.append(RMSE(y0_test, y_lr))\n",
    "    MAPE.append(mape_error(y0_test, y_lr))\n",
    "rmse, MAPE = np.array(rmse), np.array(MAPE)\n",
    "\n",
    "plt.figure().set_size_inches(8, 6)\n",
    "\n",
    "\n",
    "# plot error lines showing +/- std. errors of the scores\n",
    "\n",
    "plt.semilogx(alphas, rmse)\n",
    "#plt.semilogx(alphas, MAPE)\n",
    "\n",
    "# alpha=0.2 controls the translucency of the fill color\n",
    "\n",
    "plt.ylabel('RMSE')\n",
    "plt.xlabel('alpha')\n",
    "plt.xlim([alphas[0], alphas[-1]])\n",
    "plt.title('RMSE fonction de alpha')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#alphas qui minimise la RMSE:\n",
    "alpha=alphas[rmse.argmin()]\n",
    "#Evaluation du modèle :\n",
    "print(\"L'erreur type (RMSE) est de %s\") %((rmse.min()))\n",
    "print(\"La moyenne absolue de pourcentage d'erreur est de %s %%\") %(MAPE[rmse.argmin()])\n",
    "print(\"Meilleur model utilisant alpha = %s.\" % (alpha))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test de stabilité sur une autre coupe de test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X0_train,X0_test,y0_train,y0_test=train_test_split(X,y,test_size=0.3,random_state=1024) #nouvelle coupe\n",
    "\n",
    "lasso = Lasso(random_state=42)\n",
    "alphas = np.logspace(-4, -0.5, 30)\n",
    "\n",
    "#feature polynomial d'ordre 2\n",
    "polynomial_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X0_train_ = polynomial_features.fit_transform(X0_train)\n",
    "X0_test_ = polynomial_features.fit_transform(X0_test)\n",
    "\n",
    "#list\n",
    "rmse = list()\n",
    "MAPE = list()\n",
    "\n",
    "n_folds = 3\n",
    "\n",
    "for alpha in alphas:\n",
    "    #lasso = LassoCV(random_state=42, cv=10)\n",
    "    lasso.alpha = alpha\n",
    "    lasso.fit(X0_train_,y0_train)\n",
    "    y_lr=lasso.predict(X0_test_)\n",
    "    rmse.append(RMSE(y0_test, y_lr))\n",
    "    MAPE.append(mape_error(y0_test, y_lr))\n",
    "rmse, MAPE = np.array(rmse), np.array(MAPE)\n",
    "\n",
    "plt.figure().set_size_inches(8, 6)\n",
    "\n",
    "plt.semilogx(alphas, rmse)\n",
    "#plt.semilogx(alphas, MAPE)\n",
    "plt.ylabel('RMSE')\n",
    "plt.xlabel('alpha')\n",
    "plt.xlim([alphas[0], alphas[-1]])\n",
    "plt.title('RMSE fonction de alpha')\n",
    "\n",
    "#alphas qui minimise la RMSE:\n",
    "alpha=alphas[rmse.argmin()]\n",
    "#Evaluation du modèle :\n",
    "print(\"L'erreur type (RMSE) est de %s\") %((rmse.min()))\n",
    "print(\"La moyenne absolue de pourcentage d'erreur est de %s %%\") %(MAPE[rmse.argmin()])\n",
    "print(\"Meilleur model utilisant alpha = %s.\" % (alpha))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Le modèle est stable.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- > ***Interprétation : record pour le moment obtenu par la régression lasso sur donnée polynomiale d'ordre2.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## B. Méthodes non-linéaires : méthodes ensemblistes  <a name=\"ensemblistes\"></a>\n",
    "\n",
    "***Ici, nous allons faire usage de méthodes non-linéaires d'apprentissage automatique, et nous verrons quelle est la meilleure méthode en termes de performances (notamment en termes de taux d'erreur et de vitesse d'exécution).***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X0_train,X0_test,y0_train,y0_test=train_test_split(X,y,test_size=0.2,random_state=42) #ancienne coupe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, RandomForestRegressor, GradientBoostingRegressor \n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.externals import joblib\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Commençons par tester la méthode de régression des fôrets aléatoires.***  <a name=\"RF\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf = RandomForestRegressor(n_estimators=300, max_depth=7, n_jobs=-1, max_features=None,criterion='mse',\n",
    "                               min_samples_split=5,random_state=42)\n",
    "rf.fit(X0_train, y0_train)\n",
    "y_rf = rf.predict(X0_test)\n",
    "print(\"L'erreur type (RMSE) est de %s\") %(RMSE(y0_test, y_rf))\n",
    "print(\"La moyenne absolue de pourcentage d'erreur est de %s %%\") %(mape_error(y0_test, y_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- > ***Interprétations : résultats vraiment décevants, on va chercher à tuner un peu le model.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **GridSearch et Cross Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "rf = RandomForestRegressor(criterion='mse', max_features=None,random_state=42)\n",
    "n_estimators = [50, 100, 150, 200,300,400]\n",
    "max_depth = [2, 4, 6, 8,10]\n",
    "min_samples_split=[2,4,6,8]\n",
    "param_grid = dict(max_depth=max_depth, n_estimators=n_estimators, min_samples_split=min_samples_split)\n",
    "model = GridSearchCV(rf, param_grid, n_jobs=-1, cv=10,scoring='neg_mean_squared_error') #neg_mean_squared_error pour\n",
    "                                                                                        #sklearn > 0.18\n",
    "model.fit(X0_train, y0_train)\n",
    "y_rf = model.predict(X0_test)\n",
    "print(\"L'erreur type (RMSE) est de %s\") %(RMSE(y0_test, y_rf))\n",
    "print(\"La moyenne absolue de pourcentage d'erreur est de %s %%\") %(mape_error(y0_test, y_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Meilleur model utilisant %s.\" % ( model.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- > ***Nous obtenons une MAPE d'environ 50 % et un RMSE de 3.4. C'est légèrement mieux, mais encore une fois cela reste horrible.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Cette fois, testons toujours la méthode de régression extra-trees, mais cette fois, avec un nombre d'estimateurs plus important (porté à 300), et une profondeur maximale bien plus réduite (profondeur maximale de 7).***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "et = ExtraTreesRegressor(n_estimators=300, criterion='mse',max_depth=7,n_jobs=-1,\n",
    "                         min_samples_split=5,max_features=None)\n",
    "et.fit(X0_train, y0_train)\n",
    "y_et = et.predict(X0_test)\n",
    "print(\"L'erreur type (RMSE) est de %s\") %(RMSE(y0_test, y_et))\n",
    "print(\"La moyenne absolue de pourcentage d'erreur est de %s %%\") %(mape_error(y0_test, y_et))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **GridSearch et Cross Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "et = ExtraTreesRegressor(criterion='mse',max_features=None)\n",
    "n_estimators = [100, 150, 200,300,400]\n",
    "max_depth = [4, 6, 8,10]\n",
    "min_samples_split=[2,4,6,8]\n",
    "param_grid = dict(max_depth=max_depth, n_estimators=n_estimators, min_samples_split=min_samples_split)\n",
    "model = GridSearchCV(et, param_grid, n_jobs=-1, cv=10,scoring='neg_mean_squared_error')\n",
    "model.fit(X0_train, y0_train)\n",
    "y_et = model.predict(X0_test)\n",
    "print(\"L'erreur type (RMSE) est de %s\") %(RMSE(y0_test, y_et))\n",
    "print(\"La moyenne absolue de pourcentage d'erreur est de %s %%\") %(mape_error(y0_test, y_et))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Meilleur model utilisant %s.\" % ( model.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- > ***Nous obtenons une MAPE d'environ 36 %. L'erreur est meilleure (entendre \"plus faible\") lorsque la profondeur est importante, mais que le nombre d'estimateurs n'est pas colossal, néanmoins une grande profondeur tend à overfitter.***"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "%%time\n",
    "#Prends énormément de temps à calculer\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "rf = AdaBoostRegressor(base_estimator=DecisionTreeRegressor(max_depth=7),n_estimators=300, learning_rate=0.1,\n",
    "                       loss='linear')\n",
    "rf.fit(X0_train, y0_train)\n",
    "y_rf = rf.predict(X0_test)\n",
    "print(mape_error(y0_test, y_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Pour le XG Boost: ***<a name=\"XGB\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "xg = xgb.XGBRegressor(max_depth=3, n_estimators=300, learning_rate=0.05).fit(X0_train, y0_train)\n",
    "y_xgb = xg.predict(X0_test)\n",
    "print(\"L'erreur type (RMSE) est de %s\") %(RMSE(y0_test, y_xgb))\n",
    "print(\"La moyenne absolue de pourcentage d'erreur est de %s %%\") %(mape_error(y0_test, y_xgb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- > ***Nous obtenons un pourcentage d'erreur d'environ 24 % et une MSE de 3,3. C'est un progrès net. Le modèle XGBoost semble une piste à creuser.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "xg = xgb.XGBRegressor(seed=42,nthread=-1)\n",
    "n_estimators = [100, 150, 200,300,400]\n",
    "max_depth = [3,4, 6, 8,10]\n",
    "param_grid = dict(max_depth=max_depth, n_estimators=n_estimators)\n",
    "model = GridSearchCV(xg, param_grid, cv=10,scoring='neg_mean_squared_error') # n_jobs=-1,\n",
    "model.fit(X0_train, y0_train)\n",
    "y_xgb = model.predict(X0_test)\n",
    "print(\"L'erreur type (RMSE) est de %s\") %(RMSE(y0_test, y_xgb))\n",
    "print(\"La moyenne absolue de pourcentage d'erreur est de %s %%\") %(mape_error(y0_test, y_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "means = model.cv_results_['mean_test_score']\n",
    "stds = model.cv_results_['std_test_score']\n",
    "params = model.cv_results_['params']\n",
    "#for mean, stdev, param in zip(means, stds, params):\n",
    "#    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "# plot results\n",
    "plt.figure(figsize = (12,12))\n",
    "scores = np.array(means).reshape(len(max_depth), len(n_estimators))\n",
    "for i, value in enumerate(max_depth):\n",
    "    plt.plot(n_estimators, scores[i], label='depth: ' + str(value))\n",
    "plt.legend()\n",
    "plt.xlabel('n_estimators')\n",
    "plt.ylabel('neg_mean_squared_error')\n",
    "plt.title(\"Profondeur v.s. Nombre d'estimateurs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "xg = xgb.XGBRegressor(seed=42,nthread=-1)\n",
    "n_estimators = [100, 150, 200,300,400]\n",
    "max_depth = [3,4, 6, 8,10]\n",
    "learning_rate=[0.05,0.075]\n",
    "param_grid = dict(max_depth=max_depth, n_estimators=n_estimators,learning_rate=learning_rate)\n",
    "model = GridSearchCV(xg, param_grid, cv=10,scoring='neg_mean_squared_error') # n_jobs=-1,\n",
    "model.fit(X0_train, y0_train)\n",
    "y_xgb = model.predict(X0_test)\n",
    "print(\"L'erreur type (RMSE) est de %s\") %(RMSE(y0_test, y_xgb))\n",
    "print(\"La moyenne absolue de pourcentage d'erreur est de %s %%\") %(mape_error(y0_test, y_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Meilleur model utilisant %s.\" % ( model.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- > ***Nous obtenons une MAPE d'environ 34 % mais un RMSE des plus faibles avec 2.7. C'est très légèrement moins bien que la régression linéaire que l'on a sélectionné plus haut (degré polynomial 2, cv10 et sans normalisation).***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test du XGBoost avec des features polynomiales :**\n",
    "\n",
    "L'intuition voudrait que le fait de dériver des features polynomiales à partir de la donnée brute soit innéficace pour les méthodes de types ensemblistes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "xg = xgb.XGBRegressor(seed=42,nthread=-1)\n",
    "polynomial_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X0_train_ = polynomial_features.fit_transform(X0_train)\n",
    "X0_test_ = polynomial_features.fit_transform(X0_test)\n",
    "\n",
    "n_estimators = [100, 150, 200,300,400]\n",
    "max_depth = [3,4, 6, 8,10]\n",
    "learning_rate=[0.05,0.075]\n",
    "param_grid = dict(max_depth=max_depth, n_estimators=n_estimators,learning_rate=learning_rate)\n",
    "\n",
    "model = GridSearchCV(xg, param_grid, cv=10,scoring='neg_mean_squared_error') # n_jobs=-1,\n",
    "model.fit(X0_train_, y0_train)\n",
    "y_xgb = model.predict(X0_test_)\n",
    "\n",
    "print(\"L'erreur type (RMSE) est de %s\") %(RMSE(y0_test, y_xgb))\n",
    "print(\"La moyenne absolue de pourcentage d'erreur est de %s %%\") %(mape_error(y0_test, y_xgb))\n",
    "print(\"Meilleur model utilisant %s.\" % ( model.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- > *** Interprétation : notre intuition s'est révélée être correcte, dériver des features polynomiales n'apportent rien au modèle ensembliste.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Evaluation des modèles retenus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** On étudie les deux modèles aux plus grande performances**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - ### Modèle Linéaire :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#interprétations des coéfficients\n",
    "def pretty_print_linear(coefs, names = None, sort = False,filter_=True,threshold=0.005):\n",
    "    if names == None:\n",
    "        names = [\"X%s\" % x for x in range(len(coefs))]\n",
    "    lst = zip(coefs, names)\n",
    "    if sort:\n",
    "        lst = sorted(lst,  key = lambda x:-np.abs(x[0]))\n",
    "    \n",
    "    df=pd.DataFrame()\n",
    "    df['coef'],df['var'] = map(list, zip(*lst))\n",
    "    if filter_:\n",
    "        df = df[abs(df['coef'])>threshold]\n",
    "        df.reset_index(inplace=True,drop=True)\n",
    "    return df\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Reprise du modèle linéaire\n",
    "lr = LinearRegression() #init\n",
    "polynomial_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_ = polynomial_features.fit_transform(X)\n",
    "#X0_test_ = polynomial_features.fit_transform(X0_test)\n",
    "\n",
    "#grid = dict(linear_regression__normalize=[False,True]) #espace de paramètre de la régression\n",
    "                                                        # choix sur la normalisation\n",
    "#model = GridSearchCV(pipe,param_grid=grid,cv=8)\n",
    "lr.fit(X_, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Reconstruire le tableau avec le nom des variables\n",
    "target_feature_names = ['x'.join(['{}^{}'.format(pair[0],pair[1]) for pair in tuple if pair[1]!=0]) for tuple in [zip(X.columns,p) for p in polynomial_features.powers_]]\n",
    "output_df = pd.DataFrame(X_, columns = target_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Linear model:\"\n",
    "pretty_print_linear(lr.coef_,names = list(output_df.columns),sort=True,threshold=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "importance_frame = pretty_print_linear(lr.coef_,names = list(output_df.columns),sort=True,threshold=0.01)\n",
    "#importance_frame.sort_values(by = 'Importance', inplace = True)\n",
    "importance_frame[0:50].plot(kind = 'barh', x = 'var', figsize = (10,10), color = 'orange')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- > ***La lecture des coefficients est peu satisfante***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - ### Modèle linéaire Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lasso = Lasso(random_state=42)\n",
    "alphas = np.logspace(-4, -0.5, 30)\n",
    "\n",
    "rmse = list()\n",
    "MAPE = list()\n",
    "lr = list()\n",
    "\n",
    "n_folds = 3\n",
    "\n",
    "for alpha in alphas:\n",
    "    lasso.alpha = alpha\n",
    "    lasso.fit(X0_train_,y0_train)\n",
    "    lr.append(lasso)\n",
    "    y_lr=lasso.predict(X0_test_)\n",
    "    rmse.append(RMSE(y0_test, y_lr))\n",
    "    MAPE.append(mape_error(y0_test, y_lr))\n",
    "rmse, MAPE = np.array(rmse), np.array(MAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#alphas qui minimise la RMSE:\n",
    "alpha=alphas[rmse.argmin()]\n",
    "#Evaluation du modèle :\n",
    "best_regressor = lr[rmse.argmin()]\n",
    "\n",
    "importance_frame = pretty_print_linear(best_regressor.coef_,names = list(output_df.columns),sort=True,threshold=0.0006)\n",
    "#importance_frame.sort_values(by = 'Importance', inplace = True)\n",
    "importance_frame[0:50].plot(kind = 'barh', x = 'var', figsize = (12,12), color = 'orange')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- > ***La lecture des coefficients est bien plus satisfaisante avec la régression Lasso, les interprétations à valeurs métiers ont ici un sens.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - ### Modèle XGBoost :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Plot des features selon leur importance\n",
    "\n",
    "***Dans cette étape d'évaluation du modèle, nous allons représenter sous la forme d'un diagramme horizontal les variables qui contribuent le plus au modèle, à l'aide de la méthode XGBoost.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "dtrain = xgb.DMatrix(X0_train,label=y0_train)\n",
    "dtest = xgb.DMatrix(X0_test)\n",
    "\n",
    "params = {'booster':'gbtree', 'eta':0.2, 'max_depth':4, 'subsample':0.8, 'n_estimators':400,\n",
    "                 'silent':1, 'objective':'reg:linear', \"seed\":42, 'nhtread':12,\n",
    "                 'eval_metric':'rmse','colsample_bytree':0.7}\n",
    "    \n",
    "xg = xgb.train(params, dtrain, 400)\n",
    "y_xg = xg.predict(dtest)\n",
    "print(\"L'erreur type (RMSE) est de %s\") %(RMSE(y0_test, y_xg))\n",
    "print(\"La moyenne absolue de pourcentage d'erreur est de %s %%\") %(mape_error(y0_test, y_xg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "importances = xg.get_score()\n",
    "importance_frame = pd.DataFrame({'Importance': list(importances.values()),'Feature': list(importances.keys())})\n",
    "importance_frame.sort_values(by = 'Importance', inplace = True)\n",
    "importance_frame[0:75].plot(kind = 'barh', x = 'Feature', figsize = (12,12), color = 'orange')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- > ***L'importances des features pour le modèle XGBoost semblent être en adéquation avec le sens métier qu'elles apportent au calcul du bénifice net annuel. En effet, on peut interpréter :***\n",
    "    - Le kilométrage influe sur le risque / proba d'accidents\n",
    "    - Coût d'entretient est directement lié au bénéfice net annuel étant donné que plus la voiture est coûteuse à entretenir / réparer plus cela impactera le calcul de bénéfice (soit augmentation de la prime d'assurance, soit coût de l'assureur à réparer en cas d'accidents)\n",
    "    - etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Soumission des résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path='model/Final submit/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_to_file(X_prod,y_pred,name_file,path) :\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    X_prod['Benefice net annuel predit'] = y_pred\n",
    "    X_prod[['Benefice net annuel predit']].to_csv(path+name_file, index=True, sep='|')\n",
    "    print('Ecriture finie.')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "lr = LinearRegression() #init\n",
    "polynomial_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "# Création du pipeline\n",
    "pipe = Pipeline([(\"polynomial_features\", polynomial_features),\n",
    "        (\"linear_regression\", lr)])\n",
    "grid = dict(linear_regression__normalize=[False,True]) #espace de paramètre de la régression\n",
    "                                                        # choix sur la normalisation\n",
    "model = GridSearchCV(pipe,param_grid=grid,cv=8)\n",
    "model.fit(X, y)\n",
    "#predict\n",
    "y_lr_simple = model.predict(T)\n",
    "write_to_file(T.copy(), y_lr_simple,'ma_prediction_RL_simple.csv',path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "lasso = Lasso(random_state=42)\n",
    "\n",
    "#poly features\n",
    "polynomial_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_ = polynomial_features.fit_transform(X)\n",
    "T_ = polynomial_features.fit_transform(T)\n",
    "\n",
    "\n",
    "\n",
    "lasso.alpha = alpha #alpha minimisant la rmse sur le test d'apprentissage\n",
    "lasso.fit(X_,y)\n",
    "y_lr=lasso.predict(T_)\n",
    "\n",
    "\n",
    "\n",
    "#predicteur minimisant la\n",
    "#predict\n",
    "y_lr_lasso = model.predict(T)\n",
    "write_to_file(T.copy(), y_lr_lasso,'ma_prediction_RL_lasso.csv',path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df=pd.DataFrame()\n",
    "df['Lasso']=y_lr_lasso\n",
    "df['Simple']=y_lr_simple\n",
    "df['diff'] = df['Lasso'] -df['Simple']\n",
    "df[df['diff']!=0]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
